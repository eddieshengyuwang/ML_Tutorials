{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification using SVMs\n",
    "<br>\n",
    "In this tutorial, we will finish the SVM assignment by making our own spam email classifier. The concepts explored in this assignment also serve as a very simple introduction to some **natural language processing** techniques. Okay, let's get started by reading a provided sample email!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Anyone knows how much it costs to host a web portal ?\n",
      ">\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "if youre running something big..\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn import svm\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "with open(\"emailSample1.txt\", \"r\") as readfile:\n",
    "    email_contents_s1 = readfile.read()\n",
    "\n",
    "print(email_contents_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help build our classifier, we need some sort of vocabulary list that has the most frequent words within a whole corpus or collection of emails. Luckily the assignment provides that list along with a numberical index, which we can convert into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First  30 key-values in dictionary:\n",
      "\n",
      "aa :  1\n",
      "ab :  2\n",
      "abil :  3\n",
      "abl :  4\n",
      "about :  5\n",
      "abov :  6\n",
      "absolut :  7\n",
      "abus :  8\n",
      "ac :  9\n",
      "accept :  10\n",
      "access :  11\n",
      "accord :  12\n",
      "account :  13\n",
      "achiev :  14\n",
      "acquir :  15\n",
      "across :  16\n",
      "act :  17\n",
      "action :  18\n",
      "activ :  19\n",
      "actual :  20\n",
      "ad :  21\n",
      "adam :  22\n",
      "add :  23\n",
      "addit :  24\n",
      "address :  25\n",
      "administr :  26\n",
      "adult :  27\n",
      "advanc :  28\n",
      "advantag :  29\n",
      "advertis :  30\n"
     ]
    }
   ],
   "source": [
    "with open(\"vocab.txt\", \"r\") as readfile:\n",
    "    vocab_dict = {}\n",
    "    for line in readfile:\n",
    "        line = line.split()\n",
    "        vocab_dict[line[1]] = line[0]\n",
    "\n",
    "def printDict(dictionary, entries):\n",
    "    keys = list(dictionary.keys())\n",
    "    print(\"First \", entries, \"key-values in dictionary:\\n\")\n",
    "    for i in range(entries):\n",
    "        print(keys[i], \": \", dictionary[keys[i]])\n",
    "        \n",
    "printDict(vocab_dict,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that some of the words in `vocab_dict` are not even words! This is because the some of the \"words\" are reducted to their *stems*. For example, \"include\", \"includes\", \"included\", \"including\" all stem to \"includ\", which makes it easier for resolving those 4 \"include\"-like words into one meaning.\n",
    "\n",
    "Next we are going to preprocess the email normalize its contents a little bit. Then we are going to iterate through all the words in the email, stem the word, check if it's in `vocab_dict` and if so append the index to a vector. Essentially we are converting the text into a vector of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['86', '916', '794', '1077', '883', '370', '1699', '790', '1822', '1831', '883', '431', '1171', '794', '1002', '1893', '1364', '592', '1676', '238', '162', '89', '688', '945', '1663', '1120', '1062', '1699', '375', '1162', '479', '1893', '1510', '799', '1182', '1237', '810', '1895', '1440', '1547', '181', '1699', '1758', '1896', '688', '1676', '992', '961', '1477', '71', '530', '1699', '531']\n"
     ]
    }
   ],
   "source": [
    "def preprocessEmail(email_contents):\n",
    "    # make lowercase\n",
    "    email_contents = email_contents.lower()\n",
    "    \n",
    "    # strip HTML\n",
    "    email_contents = re.sub('<[^<>]+>', ' ', email_contents)\n",
    "    \n",
    "    # handle numbers\n",
    "    email_contents = re.sub(\"[0-9]+\", 'number', email_contents)\n",
    "    \n",
    "    # handle URLs\n",
    "    email_contents = re.sub(\"(http|https)://[^\\s]*\", \"httpaddr\", email_contents)\n",
    "    \n",
    "    # handle email addresses\n",
    "    email_contents = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email_contents)\n",
    "    \n",
    "    # handle $ sign\n",
    "    email_contents = re.sub(\"[$]+\", \"dollar\", email_contents)\n",
    "    \n",
    "    return email_contents\n",
    "\n",
    "def makeWordIndicies(email_contents):\n",
    "    '''\n",
    "    Tokenizes by whitespace and punctuation.\n",
    "    Stems word and checks if stem is in vocab_dict.\n",
    "    If so, add index to word_indices vector\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    global vocab_dict\n",
    "    word_indices = []\n",
    "    \n",
    "    # tokenize and get rid of punctuation\n",
    "    tokens = re.split(\"\"\"[\\s(\\\\\\\\)@$/#.\\-:&*+\\=[\\]?!(){},'\">_<;%]\"\"\", email_contents)\n",
    "    for token in tokens:\n",
    "        # remove non alphanumberic characters\n",
    "        token = re.sub(\"[^a-zA-Z0-9]\", '', token)\n",
    "        stem = stemmer.stem(token)\n",
    "        if len(stem) < 1:\n",
    "            continue\n",
    "        if stem in vocab_dict:\n",
    "            word_indices.append(vocab_dict[stem])\n",
    "            \n",
    "    return word_indices\n",
    "\n",
    "def processEmail(file_contents):\n",
    "    email_contents = preprocessEmail(file_contents)\n",
    "    return makeWordIndicies(email_contents)\n",
    "\n",
    "word_indices = processEmail(email_contents_s1)\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to one hot encode the above list using `vocab_dict` to create our feature vector. There are 1899 keys in `vocab_dict` so we create a 1899 length vector with 1s at the indices specified by `word_indices` and 0s everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of keys in vocab_dict:  1899\n",
      "feature vector:  [ 0.  0.  0. ...,  1.  0.  0.]\n",
      "feature vector length:  1899\n",
      "number of non-zero entries:  45.0\n"
     ]
    }
   ],
   "source": [
    "# converting to feature vector\n",
    "\n",
    "dict_n = len(vocab_dict.keys())\n",
    "print(\"number of keys in vocab_dict: \", dict_n)\n",
    "\n",
    "def emailFeatures(word_indices):\n",
    "    global dict_n\n",
    "    global vocab_dict\n",
    "    \n",
    "    featureVec = np.zeros(dict_n)\n",
    "    for i in word_indices:\n",
    "        i = int(i)\n",
    "        featureVec[i] = 1\n",
    "    return featureVec\n",
    "\n",
    "featureVec = emailFeatures(word_indices)\n",
    "print(\"feature vector: \", featureVec)\n",
    "print(\"feature vector length: \", len(featureVec))\n",
    "print(\"number of non-zero entries: \", sum(featureVec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our classifier we are going to use the provided .mat data. All the emails in the dataset are already converted from text to our defined feature vector, so all we have to do is use Scikit-learn to build our SVM linear classifier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99825\n",
      "0.989\n"
     ]
    }
   ],
   "source": [
    "spamTrain = scipy.io.loadmat(\"spamTrain.mat\")\n",
    "spamTest = scipy.io.loadmat(\"spamTest.mat\")\n",
    "\n",
    "X_train = spamTrain[\"X\"]\n",
    "y_train = spamTrain[\"y\"].ravel()\n",
    "\n",
    "X_test = spamTest[\"Xtest\"]\n",
    "y_test = spamTest[\"ytest\"].ravel()\n",
    "\n",
    "# train linear SVM classifier\n",
    "\n",
    "C = 0.1\n",
    "clf = svm.SVC(C=C, kernel=\"linear\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "def svmPredict(model, X):\n",
    "    # returns predictions\n",
    "    predictions = clf.predict(X)\n",
    "    return predictions.ravel()\n",
    "\n",
    "def getAccuracy(predictions, y):\n",
    "    logicalVec = predictions == y\n",
    "    logicalVec = logicalVec.astype(int)\n",
    "    return sum(logicalVec) / len(logicalVec)\n",
    "\n",
    "predictions_train = svmPredict(clf, X_train)\n",
    "print(getAccuracy(predictions_train, y_train))\n",
    "\n",
    "predictions_test = svmPredict(clf, X_test)\n",
    "print(getAccuracy(predictions_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eh, the accuracy is close enough (in fact higher) to the ones indicated on the assignment. This may be because Scikit-learn's implementation of building and training an SVM classier is different than Octave's. Next we will look at the top 15 predictor words for spam in an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 predictor words for spam:\n",
      "- otherwis\n",
      "- clearli\n",
      "- remot\n",
      "- gt\n",
      "- visa\n",
      "- base\n",
      "- doesn\n",
      "- wife\n",
      "- previous\n",
      "- player\n",
      "- mortgag\n",
      "- natur\n",
      "- ll\n",
      "- futur\n",
      "- hot\n"
     ]
    }
   ],
   "source": [
    "highestInd = np.argsort(-clf.coef_)[0]\n",
    "# argsort can't sort by desc, so negate clf.coef_ for to get sorted by desc\n",
    "\n",
    "# switch key, values for vocab_dict (key-values will be int: str)\n",
    "vocab_dict_switch = {y:x for x,y in vocab_dict.items()}\n",
    "\n",
    "print('Top 15 predictor words for spam:')\n",
    "for i in range(15):\n",
    "    index = highestInd[i]\n",
    "    word = vocab_dict_switch[str(index)]\n",
    "    print(\"- \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the words are different than the ones indicated on the assignment. I hypothesize this is because of different SVM implementations between Scikit-learn and Octave's SVM library. If anyone did get the same words I'd be happy to know how you got them!\n",
    "\n",
    "Okay, let's test our spam detector out with the sample emails provided in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emailSample1.txt  is predicted to not be spam.\n",
      "spamSample1.txt  is predicted to not be spam.\n",
      "emailSample2.txt  is predicted to not be spam.\n",
      "spamSample2.txt  is predicted to be spam.\n"
     ]
    }
   ],
   "source": [
    "def detectIfSpam(email_content, clf):\n",
    "    wordIndices = processEmail(email_content)\n",
    "    featureVec = emailFeatures(wordIndices).reshape(1,-1) # single email, reshape to 2D array\n",
    "    prediction = svmPredict(clf, featureVec)    \n",
    "    return prediction\n",
    "\n",
    "def exampleEmailTest(clf):\n",
    "    emails = [\"emailSample1.txt\", \"spamSample1.txt\", \"emailSample2.txt\", \"spamSample2.txt\"]\n",
    "\n",
    "    for email in emails:\n",
    "        with open(email, \"r\") as readfile:\n",
    "            content = readfile.read()\n",
    "        result = detectIfSpam(content, clf)\n",
    "        if result == 1:\n",
    "            print(email, \" is predicted to be spam.\")\n",
    "        else:\n",
    "            print(email, \" is predicted to not be spam.\")\n",
    "    return\n",
    "\n",
    "exampleEmailTest(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like all the classifications are correct except spamSample1. Below is an example of a spam email from my own inbox. Let's put it to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "Our VIP department is trying to contact eddiewang \n",
    "You could win millions\n",
    "\n",
    "7 dollars BONUS TO TRY\n",
    "\n",
    ", Claim your BONUS now!\n",
    "\n",
    "\n",
    "I want to claim my 70 FREE SPINS !\n",
    "\"\"\"\n",
    "\n",
    "print(detectIfSpam(txt, clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice it predicts correctly.\n",
    "\n",
    "That is it for the SVM assignment, congrats for making it through - just two more assignments left! In the next post we will move away from supervised learning techniques but instead begin to look at unsupervised learning using k-means clustering. See you there! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
