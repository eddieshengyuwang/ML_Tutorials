{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression Tutorial\n",
    "<br>\n",
    "In the previous tutorial, we predicted restaurant profits (our **label**) using linear regression based on a sole **feature**, which was the population of cities that restaurants were located in. However at often times, there are more than one features that affect the label, for example size of the restaurant in square feet, number of competitors within its vicinity, etc. In this situation, we use **Mutivariate Linear Regression** which occurs when we want to perform linear regression when our data set has more than one feature. \n",
    "\n",
    "Note that since the dataset is multivariate, we are dealing with **multi-dimensional** data, which cannot be represented on a 2D graph. The best we can do for visualization is to use a 3D graph, but that is only in the special case that there are two features and one label (ie. predicting restaurant profits based on only two factors, like population of city and size of restaurant). However in practice, often times there are more than two features that determine a label, which makes it difficult to depict our data in a graph.\n",
    "\n",
    "For this tutorial, we will use the dataset \"ex1data2.txt\" found in this directory to predict housing prices. The dataset comes with three columns: Size of house (in square feet), number of bedrooms, and price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Once again, the first step is to import libraries and load our DataFrame. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) # Don't print in scientific notation\n",
    "\n",
    "# although our data is 3D, we are not going visualize our dataset/prediction\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('ex1data2.txt', names = ['Size of house (sq ft)',\n",
    "                                          'Number of bedrooms',\n",
    "                                          'Price of house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multivariate linear regression, we need to introduce some new notation:\n",
    "- m is the number of data points (46 in this example)\n",
    "- n is the number of features (2 in this example)\n",
    "- $x^{(i)}_j$ is the value at the j-th feature on the i-th training example ($x^{(43)}_1$ = 1200, 1st feature on 43rd data point)\n",
    "- $y^{(i)}$ is the i-th label ($y^{(4)}$ = 539900)\n",
    "\n",
    "<br>\n",
    "Recall the equation of a line for SLRM:\n",
    "$$ y = \\theta_0x_0 + \\theta_1x_1 = \\theta_0 + \\theta_1x_1 $$\n",
    "\n",
    "For multivariate linear regression, our new equation (which we will call the hypothesis function) is: \n",
    "<br>\n",
    "<br>\n",
    "\\begin{align*}\n",
    "h(x) & = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + ... + \\theta_nx_n &&& \\text{note: }x_0 = 1 \\\\\\\\\n",
    "    & = \\left [  \\theta_0~\\theta_1~...~\\theta_n\\right ]\\begin{bmatrix}\n",
    "x_0\\\\ \n",
    "x_1\\\\ \n",
    "...\\\\\n",
    "x_n \n",
    "\\end{bmatrix} \\\\\n",
    " & = \\theta^Tx\n",
    "\\end{align*}\n",
    "<br>\n",
    "<br>\n",
    "Similarly, recall the gradient descent algorithm for SLRM that for each iteration we set:\n",
    "$$ \\theta_0 = \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(\\theta_0 + \\theta_1x^{(i)} - y^{(i)}) \\\\ \\theta_1 = \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(\\theta_0 + \\theta_1x^{(i)} - y^{(i)})~ x^{(i)} $$\n",
    "\n",
    "However for multiple variables, for each iteration in the gradient descent algorithm, we set:\n",
    "\\begin{align*}\n",
    "\\theta_0 &= \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(\\theta_0 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + ... + \\theta_nx^{(i)}_n - y^{(i)}) = \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (\\theta^Tx^{(i)} - y^{(i)}) \\\\\n",
    "\\theta_1 &= \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(\\theta^Tx^{(i)} - y^{(i)})~ x^{(i)}_1 \\\\\n",
    "\\theta_2 &= \\theta_2 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(\\theta^Tx^{(i)} - y^{(i)})~ x^{(i)}_2 \\\\ \n",
    "& =~... \\\\\n",
    "\\theta_n &= \\theta_n - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(\\theta^Tx^{(i)} - y^{(i)})~ x^{(i)}_n \\\\\n",
    "\\end{align*}\n",
    "<br>\n",
    "The last thing to note about multivariate linear regression is the concept of **feature scaling**. For this dataset, notice that the size of house (sq ft) varies greatly vs the number of bedrooms as shown below. The values of house sizes are also much larger than values for the number of bedrooms column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Size of house (sq ft)  Number of bedrooms  Price of house\n",
      "0                   2104                   3          399900\n",
      "1                   1600                   3          329900\n",
      "2                   2400                   3          369000\n",
      "3                   1416                   2          232000\n",
      "4                   3000                   4          539900\n",
      "\n",
      "\n",
      "    Size of house (sq ft)  Number of bedrooms  Price of house\n",
      "42                   2567                   4          314000\n",
      "43                   1200                   3          299000\n",
      "44                    852                   2          179900\n",
      "45                   1852                   4          299900\n",
      "46                   1203                   3          239500\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))\n",
    "print('\\n')\n",
    "print(df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This can cause a problem when modelling the regression. Because the house size column has higher values and variance than the number of bedrooms column, the algorithm may weigh house size as a heavier factor vs number of bedrooms in determining house price. Thus, we need to scale every feature proportionally to avoid this bias.\n",
    "\n",
    "Another advantage of using feature scaling is that gradient descent will converge faster. Smaller ranges converge faster vs. larger rangers, therefore a mix of small and large ranges without feature scaling passed into the gradient descent algorithm will cause inefficient oscillation towards finding the minimum.\n",
    "\n",
    "To do feature scaling, we will try to restrict every value in our DataFrame between [-1, 1] as much as possible. The technique to do this is called **standardization**, which is to adjust input values $x^{(i)}$ such that:\n",
    "$$ x^{(i)} := \\frac{x^{(i)} - \\mu_i}{s_i} $$\n",
    "where $\\mu_i$ and $s_i$ are the mean and standard deviation of all data points for feature i respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Size of house (sq ft)  Number of bedrooms  Price of house\n",
      "0               0.130010           -0.223675        0.475747\n",
      "1              -0.504190           -0.223675       -0.084074\n",
      "2               0.502476           -0.223675        0.228626\n",
      "3              -0.735723           -1.537767       -0.867025\n",
      "4               1.257476            1.090417        1.595389\n"
     ]
    }
   ],
   "source": [
    "def standardize(df):\n",
    "    for column in df.columns:\n",
    "        columnMean = df[column].mean()\n",
    "        columnStd = df[column].std()\n",
    "        df[column] = df[column].apply(lambda x: (x-columnMean)/columnStd)\n",
    "    return df\n",
    "\n",
    "df = standardize(df)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_matrix = df.as_matrix(df.columns[:-1])\n",
    "X_matrix = np.insert(X_matrix, 0, 1, axis=1) # remember to insert column of 1's for X_0\n",
    "X_matrix = np.matrix(X_matrix)\n",
    "y = df[df.columns[-1:]].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the methods for ```computeCost(...)``` and ```gradientDescent(...)``` from the last tutorial. One advantage of using **vectorization** is that we don't need to add extra code to these methods as long as X, y, and theta matrix dimensions align properly for matrix operations. Thus, we can keep our code from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: \n",
      "[[-0.        ]\n",
      " [ 0.88404235]\n",
      " [-0.05245518]]\n",
      "\n",
      " J_history:\n",
      "[[ 0.4805491 ]\n",
      " [ 0.47198588]\n",
      " [ 0.46366462]\n",
      " ..., \n",
      " [ 0.13068671]\n",
      " [ 0.13068671]\n",
      " [ 0.13068671]]\n"
     ]
    }
   ],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    m = len(y)\n",
    "    cost = np.sum(np.square(X*theta - y))/(2*m)\n",
    "    return cost\n",
    "\n",
    "numCols = df.shape[1]\n",
    "theta = np.matrix(np.zeros((numCols,1))) \n",
    "# first initialize theta to be zeros\n",
    "\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = np.zeros((num_iters,1)) \n",
    "    # keep track of the value of each error as we iterate\n",
    "    \n",
    "    for i in range(num_iters):  \n",
    "        \n",
    "        error = X*theta - y\n",
    "        theta = theta - (alpha/m) * (X.T * error)\n",
    "        \n",
    "        J_history[[i]] = computeCost(X,y,theta)\n",
    "        # To see value of cost at each iteration\n",
    "    \n",
    "    return (theta, J_history)\n",
    "\n",
    "result = gradientDescent(X_matrix,y,theta,alpha,iterations)\n",
    "theta = result[0]\n",
    "J_history = result[1]\n",
    "\n",
    "print('theta: ')\n",
    "print(theta)\n",
    "print('\\n J_history:')\n",
    "print(J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm returned the optimal $(\\theta_0,\\theta_1,\\theta_2)$  values (0, 0.884, -0.052) that gave a final mininmal cost of around 0.1307."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
