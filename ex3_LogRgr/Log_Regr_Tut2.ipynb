{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Tutorial Pt. 2\n",
    "<br>\n",
    "In the second half of the assignment, we will be working with Logistic Regression on another dataset which deals with microchip testing. We are given training data with two features, \"Microchip test 1\" and \"Microchip test 2\", and a label column \"Accepted\" deeming if the chip was accepted based on the two tests. Let's start off with visualizing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Microchip Test 1  Microchip Test 2  Accepted\n",
      "0    0.051267          0.699560          1       \n",
      "1   -0.092742          0.684940          1       \n",
      "2   -0.213710          0.692250          1       \n",
      "3   -0.375000          0.502190          1       \n",
      "4   -0.513250          0.465640          1       \n",
      "..        ...               ...         ..       \n",
      "113 -0.720620          0.538740          0       \n",
      "114 -0.593890          0.494880          0       \n",
      "115 -0.484450          0.999270          0       \n",
      "116 -0.006336          0.999270          0       \n",
      "117  0.632650         -0.030612          0       \n",
      "\n",
      "[118 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK+bSTEA3FgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111bb0898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "df = pd.read_csv('ex2data2.txt', names=[\"Microchip Test 1\", \"Microchip Test 2\", \"Accepted\"])\n",
    "print(df)\n",
    "\n",
    "accepted = df[df[\"Accepted\"] == 1]\n",
    "notAccepted = df[df[\"Accepted\"] == 0]\n",
    "\n",
    "acceptedTest1 = accepted[\"Microchip Test 1\"] # x axis\n",
    "acceptedTest2 = accepted[\"Microchip Test 2\"] # y axis\n",
    "\n",
    "notAcceptedTest1 = notAccepted[\"Microchip Test 1\"] # x axis\n",
    "notAcceptedTest2 = notAccepted[\"Microchip Test 2\"] # y axis\n",
    "\n",
    "plt.scatter(acceptedTest1, acceptedTest2, marker='x', label=\"Accepted\")\n",
    "plt.scatter(notAcceptedTest1, notAcceptedTest2, marker='o', label=\"Not accepted\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Microchip Test 1\")\n",
    "plt.ylabel(\"Microchip Test 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this dataset's decision boundary cannot be simply represented by a line. We will need to map a more complex polynomial equation to accurately determine this dataset's decision boundary. Thus, we will map the features ($x_1, x_2$) to be polynomial terms up to the sixth-power so that we can come up with this more complex decison boundary:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "     mapFeatures(x) = mapFeatures(x_1,x_2) =\\begin{bmatrix}\n",
    "         1 \\\\\n",
    "         x_1 \\\\\n",
    "         x_2 \\\\\n",
    "         x_1^2 \\\\\n",
    "         x_1x_2 \\\\\n",
    "         x_2^2 \\\\\n",
    "         ... \\\\\n",
    "         x_1x_2^5 \\\\\n",
    "         x_2^6\n",
    "        \\end{bmatrix}\n",
    "  \\end{equation} $$\n",
    "  \n",
    "This means that for every feature row in the DataFrame, we will transform that row so that it reflects the new polynomial terms described above. That is, we will compute $1, x_1, x_2, x_1^2, x_1x_2, ... , x_1x_2^5, x_2^6$ and that will be a row for our transformed X data. In total there should be still 118 rows but the number of columns increase to 28 since we are adding the new computed terms. We will leverage *sci-kit learn*'s tools to easily come up with our transformed X matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1         2         3         4      ...                 23  \\\n",
      "0    1.0  0.051267  0.699560  0.002628  0.035864      ...       3.380660e-06   \n",
      "1    1.0 -0.092742  0.684940  0.008601 -0.063523      ...       3.470651e-05   \n",
      "2    1.0 -0.213710  0.692250  0.045672 -0.147941      ...       9.995978e-04   \n",
      "3    1.0 -0.375000  0.502190  0.140625 -0.188321      ...       4.987251e-03   \n",
      "4    1.0 -0.513250  0.465640  0.263426 -0.238990      ...       1.504584e-02   \n",
      "..   ...       ...       ...       ...       ...      ...                ...   \n",
      "113  1.0 -0.720620  0.538740  0.519293 -0.388227      ...       7.826790e-02   \n",
      "114  1.0 -0.593890  0.494880  0.352705 -0.293904      ...       3.046659e-02   \n",
      "115  1.0 -0.484450  0.999270  0.234692 -0.484096      ...       5.499985e-02   \n",
      "116  1.0 -0.006336  0.999270  0.000040 -0.006332      ...       1.609667e-09   \n",
      "117  1.0  0.632650 -0.030612  0.400246 -0.019367      ...       1.501196e-04   \n",
      "\n",
      "               24            25            26            27  \n",
      "0    4.613055e-05  6.294709e-04  8.589398e-03  1.172060e-01  \n",
      "1   -2.563226e-04  1.893054e-03 -1.398103e-02  1.032560e-01  \n",
      "2   -3.237900e-03  1.048821e-02 -3.397345e-02  1.100469e-01  \n",
      "3   -6.678793e-03  8.944062e-03 -1.197765e-02  1.604015e-02  \n",
      "4   -1.365016e-02  1.238395e-02 -1.123519e-02  1.019299e-02  \n",
      "..            ...           ...           ...           ...  \n",
      "113 -5.851357e-02  4.374511e-02 -3.270412e-02  2.444980e-02  \n",
      "114 -2.538737e-02  2.115493e-02 -1.762810e-02  1.468924e-02  \n",
      "115 -1.134476e-01  2.340073e-01 -4.826843e-01  9.956280e-01  \n",
      "116 -2.538495e-07  4.003286e-05 -6.313306e-03  9.956280e-01  \n",
      "117 -7.263830e-06  3.514745e-07 -1.700678e-08  8.229060e-10  \n",
      "\n",
      "[118 rows x 28 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=6) \n",
    "\n",
    "X_data = df[df.columns[:-1]]\n",
    "y_data = df[df.columns[-1]]\n",
    "\n",
    "pd.set_option('display.max_columns', 10)  \n",
    "transformedX_data = poly.fit_transform(X_data)\n",
    "\n",
    "print(pd.DataFrame(transformedX_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if our transformedX_data matrix makes sense. \n",
    "<br>\n",
    "The 1st column = 1, the 2nd column = $x_1$, the 3rd column = $x_2$, the 4th column = $x_1^2$, the 5th column = $x_1x_2$, ... , the 28th column = $x_2^6$. \n",
    "<br><br>\n",
    "If we take the first row for example, ($x_1 = 0.051267, x_2 = 0.699560$) we can quickly check that: <br><br>\n",
    "$$x_1x_2 = 0.051267 \\times 0.699560 = 0.03586~ ~\\text{(5th column)} \\\\  \n",
    "x_2^6 = 0.669560^6 = 1.172060e-01 ~ ~\\text{(28th column)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our transformed matrix, we can now move forward with the logistic regression algorithm and fit our theta parameters that will minimize our objective cost function by once again using the gradient and advanced optimization techniques. However this time, the assignment asks for us to incorporate **regularization** into our cost function and gradient calculation. First, let's introduce the concept of **overfitting/underfitting**. \n",
    "\n",
    "![](https://raw.githubusercontent.com/alexeygrigorev/wiki-figures/master/ufrt/kddm/overfitting-logreg-ex.png)\n",
    "\n",
    "**Overfitting** occurs when the determined model is complex to the point that it fits the training data too well. So although the model fits the training data extrememly well, the model will likely not perform well for new test data since the model caters to the training data too well. **Underfitting** is the opposite, which happens when the model is too simple and cannot fit the data well. In machine learning, we strive to find the balance between overfitting and underfitting data, but in practice this is very hard to do.\n",
    "\n",
    "One way to reduce overfitting is to incorporate **regularization**. Say we want to make the following expression more linear: $$ \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3 + \\theta_4x^4$$\n",
    "\n",
    "Thus we want to reduce the influence of the $\\theta_2x^2, \\theta_3x^3, \\theta_4x^4$ terms. We do this by modifying our cost function (for linear regression) like so:\n",
    "\n",
    "$$\n",
    "cost = \\frac{1}{2m} \\sum_{i = 1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + 5000\\theta_2^2 + 5000\\theta_3^3 + 5000\\theta_4^4\n",
    "$$\n",
    "\n",
    "Since we want to find theta values that will minimize the cost function, $\\theta_2, \\theta_3, \\theta_4$ are going to be very small values since they have been heavily \"inflated\" by a factor of 5000. Thus we have regularized the equation by reducing the complexity of the expression (by reducing the influence of $\\theta_2$) and thus reduced overfitting!\n",
    "\n",
    "The regularized cost function for logistic regression is generalized using all the $\\theta$ terms **except $\\theta_0$** through the following equation:\n",
    "\n",
    "$$\n",
    "cost = \\frac{1}{m} \\sum_{i = 1}^m -y^{(i)}\\log{}h_\\theta(x^{(i)}) - (1-y^{(i)})(1-\\log{}h_\\theta(x^{(i)})) + \\frac{\\lambda}{2m}\\sum_{j = 1}^m \\theta_j^2\n",
    "$$\n",
    "\n",
    "Note that $\\lambda$ is called the **regularization parameter** and it determines how much to inflate our $\\theta$ parameters. Thus, higher $\\lambda$ values will reduce overfitting by a bigger measure (perhaps to the point that it will underfit) whereas low $\\lambda$ or $\\lambda = 0$ means no regularization which entails the possibility of overfitting. Okay time to implement the regularizedCost function for logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): # same as before\n",
    "    return 1.0/(1+ math.exp(-x))\n",
    "\n",
    "def sigmoidMatrix(matrix): # same as before\n",
    "    for x in np.nditer(matrix, op_flags=['readwrite']):\n",
    "        x[...]=sigmoid(x)\n",
    "    return matrix\n",
    "\n",
    "def regularizedCost(theta,X,y,lamda):\n",
    "    theta_with_0_as_first = np.insert(theta[1:], 0, 0) \n",
    "    # important! don't want to incorporate theta_0 for the regularization term\n",
    "    # thus substitute theta_0 with 0 and use this variable for our reg term\n",
    "    \n",
    "    theta = np.matrix(theta)\n",
    "    m = len(y)\n",
    "    h = sigmoidMatrix(X*theta.T)\n",
    "    \n",
    "    reg_term = lamda*np.sum(theta_with_0_as_first) / (2*m)\n",
    "    \n",
    "    cost = (-y.T * np.log(h) - (1-y).T * np.log(1-h))/m + reg_term\n",
    "    return cost[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros(transformedX_data.shape[1])\n",
    "lamda = 1\n",
    "y_data = np.asmatrix(y_data).T\n",
    "\n",
    "print(regularizedCost(theta,transformedX_data, y_data, lamda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.47457627e-03]\n",
      " [  1.87880932e-02]\n",
      " [  7.77711864e-05]\n",
      " [  5.03446395e-02]\n",
      " [  1.15013308e-02]\n",
      " [  3.76648474e-02]\n",
      " [  1.83559872e-02]\n",
      " [  7.32393391e-03]\n",
      " [  8.19244468e-03]\n",
      " [  2.34764889e-02]\n",
      " [  3.93486234e-02]\n",
      " [  2.23923907e-03]\n",
      " [  1.28600503e-02]\n",
      " [  3.09593720e-03]\n",
      " [  3.93028171e-02]\n",
      " [  1.99707467e-02]\n",
      " [  4.32983232e-03]\n",
      " [  3.38643902e-03]\n",
      " [  5.83822078e-03]\n",
      " [  4.47629067e-03]\n",
      " [  3.10079849e-02]\n",
      " [  3.10312442e-02]\n",
      " [  1.09740238e-03]\n",
      " [  6.31570797e-03]\n",
      " [  4.08503006e-04]\n",
      " [  7.26504316e-03]\n",
      " [  1.37646175e-03]\n",
      " [  3.87936363e-02]]\n"
     ]
    }
   ],
   "source": [
    "def gradient(theta, X,y,lamda):\n",
    "    theta_with_0_as_first = np.insert(theta[1:],0,0)\n",
    "    theta = np.matrix(theta)\n",
    "    theta_with_0_as_first = np.matrix(theta_with_0_as_first)\n",
    "    m = len(y)\n",
    "    error = sigmoidMatrix(X*theta.T) - y\n",
    "    gradient = (X.T * error) / m + (lamda/m) * theta_with_0_as_first.T\n",
    "    return gradient\n",
    "\n",
    "print(gradient(theta,transformedX_data, y_data, lamda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.16450933162\n",
      "[[ 0.34604507]\n",
      " [ 0.16135192]\n",
      " [ 0.19479576]\n",
      " [ 0.22686278]\n",
      " [ 0.09218568]\n",
      " [ 0.24438558]\n",
      " [ 0.14339212]\n",
      " [ 0.10844171]\n",
      " [ 0.10231439]\n",
      " [ 0.18346846]\n",
      " [ 0.17353003]\n",
      " [ 0.08725552]\n",
      " [ 0.11822776]\n",
      " [ 0.0858433 ]\n",
      " [ 0.19994895]\n",
      " [ 0.13522653]\n",
      " [ 0.09497527]\n",
      " [ 0.09356441]\n",
      " [ 0.09979784]\n",
      " [ 0.09140157]\n",
      " [ 0.17485242]\n",
      " [ 0.14955442]\n",
      " [ 0.08678566]\n",
      " [ 0.09897686]\n",
      " [ 0.08531951]\n",
      " [ 0.10190666]\n",
      " [ 0.08450198]\n",
      " [ 0.18228323]]\n",
      "0.494537069667\n",
      "[[ 0.32617845 -0.00816167  0.16577566 -0.4467107  -0.11177062 -0.27891206\n",
      "  -0.07146233 -0.0578822  -0.06509221 -0.10636616 -0.33673632 -0.01297725\n",
      "  -0.11670702 -0.02811068 -0.28602736 -0.11614874 -0.03704565 -0.02242813\n",
      "  -0.04885919 -0.04163398 -0.18675511 -0.25334225 -0.00291228 -0.0579624\n",
      "  -0.00053023 -0.0635291  -0.01206538 -0.27148738]]\n"
     ]
    }
   ],
   "source": [
    "lamda = 10\n",
    "theta = np.ones(transformedX_data.shape[1])\n",
    "\n",
    "print(regularizedCost(theta,transformedX_data,y_data,lamda))\n",
    "\n",
    "print(gradient(theta,transformedX_data, y_data,lamda))\n",
    "\n",
    "# lamda = 1\n",
    "# theta = np.ones(transformedX_data.shape[1])\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "minimum = fmin_tnc(func=regularizedCost, x0=theta, fprime=gradient, args=(transformedX_data, y_data, lamda))\n",
    "cost = regularizedCost(minimum[0],transformedX_data,y_data,lamda)\n",
    "new_theta = np.matrix(minimum[0])\n",
    "print (cost)\n",
    "print (new_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27]\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.arange(transformedX_data.shape[1])\n",
    "print(theta)\n",
    "theta = np.matrix(theta)\n",
    "print(theta)\n",
    "theta[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
